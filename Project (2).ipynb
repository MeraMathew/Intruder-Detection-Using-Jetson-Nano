{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9a42f6a-b553-4a1a-b2c0-eab6a771e739",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading annotations...\n",
      "Annotations downloaded.\n",
      "Target class IDs: [1, 27, 31, 33, 77, 73]\n",
      "Found 67910 images with target classes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c8490f967248399c9deefab15fc788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images:   0%|          | 0/67910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000262145.jpg http://images.cocodataset.org/train2017/000000262145.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import urllib.request\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#Set up folders\n",
    "base_dir = os.path.expanduser(\"~/intruder_dataset\")\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "img_dir = os.path.join(base_dir, \"images\")\n",
    "ann_dir = os.path.join(base_dir, \"annotations\")\n",
    "os.makedirs(img_dir, exist_ok=True)\n",
    "os.makedirs(ann_dir, exist_ok=True)\n",
    "#Download annotations JSON\n",
    "ann_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "ann_zip = os.path.join(ann_dir, \"annotations_trainval2017.zip\")\n",
    "if not os.path.exists(ann_zip):\n",
    "    print(\"Downloading annotations...\")\n",
    "    urllib.request.urlretrieve(ann_url, ann_zip)\n",
    "    print(\"Annotations downloaded.\")\n",
    "\n",
    "#Unzip annotations\n",
    "import zipfile\n",
    "with zipfile.ZipFile(ann_zip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(ann_dir)\n",
    "#Load the annotation JSON\n",
    "with open(os.path.join(ann_dir, \"annotations/instances_train2017.json\")) as f:\n",
    "    coco = json.load(f)\n",
    "#Define target categories\n",
    "target_classes = [\"person\", \"backpack\", \"handbag\", \"suitcase\", \"cell phone\", \"laptop\"]\n",
    "\n",
    "# Map class names to IDs\n",
    "category_id_map = {cat[\"name\"]: cat[\"id\"] for cat in coco[\"categories\"]}\n",
    "target_ids = [category_id_map[name] for name in target_classes if name in category_id_map]\n",
    "print(\"Target class IDs:\", target_ids)\n",
    "#Get image IDs that contain those classes\n",
    "image_ids = set()\n",
    "for ann in coco[\"annotations\"]:\n",
    "    if ann[\"category_id\"] in target_ids:\n",
    "        image_ids.add(ann[\"image_id\"])\n",
    "#corresponding image metadata\n",
    "image_map = {img[\"id\"]: img for img in coco[\"images\"]}\n",
    "target_images = [image_map[i] for i in image_ids]\n",
    "print(f\"Found {len(target_images)} images with target classes.\")\n",
    "\n",
    "#Download images one-by-one\n",
    "base_img_url = \"http://images.cocodataset.org/train2017/\"\n",
    "for img in tqdm(target_images, desc=\"Downloading images\"):\n",
    "    filename = img[\"file_name\"]\n",
    "    img_url = base_img_url + filename\n",
    "    dest_path = os.path.join(img_dir, filename)\n",
    "    if not os.path.exists(dest_path):\n",
    "        urllib.request.urlretrieve(img_url, dest_path)\n",
    "    print( filename, img_url)\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07e14f30-5325-4be5-b8aa-92f82f45a655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Target class IDs: [1, 27, 31, 33, 77, 73]\n",
      "Found 67910 images with target classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67910/67910 [00:01<00:00, 37288.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import urllib.request\n",
    "from tqdm import tqdm  \n",
    "\n",
    "# Step 1: Set up folders\n",
    "base_dir = os.path.expanduser(\"~/intruder_dataset\")\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "img_dir = os.path.join(base_dir, \"images\")\n",
    "ann_dir = os.path.join(base_dir, \"annotations\")\n",
    "os.makedirs(img_dir, exist_ok=True)\n",
    "os.makedirs(ann_dir, exist_ok=True)\n",
    "\n",
    "# Step 2: Download annotations JSON\n",
    "ann_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "ann_zip = os.path.join(ann_dir, \"annotations_trainval2017.zip\")\n",
    "\n",
    "if not os.path.exists(ann_zip):\n",
    "    print(\"Downloading annotations...\")\n",
    "    urllib.request.urlretrieve(ann_url, ann_zip)\n",
    "    print(\"Annotations downloaded.\")\n",
    "\n",
    "# Step 3: Unzip annotations\n",
    "import zipfile\n",
    "with zipfile.ZipFile(ann_zip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(ann_dir)\n",
    "\n",
    "# Step 4: Load the annotation JSON\n",
    "with open(os.path.join(ann_dir, \"annotations/instances_train2017.json\")) as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# Step 5: Define target categories\n",
    "target_classes = [\"person\", \"backpack\", \"handbag\", \"suitcase\", \"cell phone\", \"laptop\"]\n",
    "\n",
    "# Map class names to IDs\n",
    "category_id_map = {cat[\"name\"]: cat[\"id\"] for cat in coco[\"categories\"]}\n",
    "target_ids = [category_id_map[name] for name in target_classes if name in category_id_map]\n",
    "\n",
    "print(\"Target class IDs:\", target_ids)\n",
    "\n",
    "# Step 6: Get image IDs that contain those classes\n",
    "image_ids = set()\n",
    "for ann in coco[\"annotations\"]:\n",
    "    if ann[\"category_id\"] in target_ids:\n",
    "        image_ids.add(ann[\"image_id\"])\n",
    "\n",
    "# Step 7: Get corresponding image metadata\n",
    "image_map = {img[\"id\"]: img for img in coco[\"images\"]}\n",
    "target_images = [image_map[i] for i in image_ids]\n",
    "\n",
    "print(f\"Found {len(target_images)} images with target classes.\")\n",
    "\n",
    "# Step 8: Download images one-by-one\n",
    "base_img_url = \"http://images.cocodataset.org/train2017/\"\n",
    "for img in tqdm(target_images, desc=\"Downloading images\"):\n",
    "    filename = img[\"file_name\"]\n",
    "    img_url = base_img_url + filename\n",
    "    dest_path = os.path.join(img_dir, filename)\n",
    "    if not os.path.exists(dest_path):\n",
    "        urllib.request.urlretrieve(img_url, dest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f03d952-ab3b-454d-87e6-47bc0b751cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67910/67910 [00:10<00:00, 6236.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO annotations successfully converted to YOLO format!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set your base dataset path (ensure this points to where your images and annotations are stored)\n",
    "base_dir = os.path.expanduser(\"~/intruder_dataset\")\n",
    "img_dir = os.path.join(base_dir, \"images\")\n",
    "ann_dir = os.path.join(base_dir, \"annotations\")\n",
    "\n",
    "# Create the output labels directory\n",
    "label_dir = os.path.join(base_dir, \"labels\")\n",
    "os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "# Load the annotation JSON file\n",
    "with open(os.path.join(ann_dir, \"annotations/instances_train2017.json\")) as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# Define target class IDs (this part should be done already, assuming you've filtered for your classes)\n",
    "target_classes = [\"person\", \"backpack\", \"handbag\", \"suitcase\", \"cell phone\", \"laptop\"]\n",
    "category_id_map = {cat[\"name\"]: cat[\"id\"] for cat in coco[\"categories\"]}\n",
    "target_ids = [category_id_map[name] for name in target_classes if name in category_id_map]\n",
    "\n",
    "# Build mapping from COCO category_id to YOLO class index\n",
    "target_id_to_index = {cat_id: idx for idx, cat_id in enumerate(target_ids)}\n",
    "\n",
    "# Create a map from image_id to annotations\n",
    "ann_map = defaultdict(list)\n",
    "for ann in coco[\"annotations\"]:\n",
    "    if ann[\"category_id\"] in target_ids:\n",
    "        ann_map[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "# Get the images that we filtered earlier (the ones matching target classes)\n",
    "image_map = {img[\"id\"]: img for img in coco[\"images\"]}\n",
    "target_images = [image_map[i] for i in ann_map.keys()]\n",
    "\n",
    "# Convert each annotation to YOLO format\n",
    "for img in tqdm(target_images, desc=\"Converting annotations\"):\n",
    "    img_id = img[\"id\"]\n",
    "    file_name = os.path.splitext(img[\"file_name\"])[0]\n",
    "    width, height = img[\"width\"], img[\"height\"]\n",
    "\n",
    "    yolo_lines = []\n",
    "    for ann in ann_map[img_id]:\n",
    "        x, y, w, h = ann[\"bbox\"]\n",
    "        x_center = (x + w / 2) / width\n",
    "        y_center = (y + h / 2) / height\n",
    "        w_norm = w / width\n",
    "        h_norm = h / height\n",
    "        class_id = target_id_to_index[ann[\"category_id\"]]\n",
    "        yolo_lines.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\")\n",
    "\n",
    "    # Write to .txt file\n",
    "    label_path = os.path.join(label_dir, file_name + \".txt\")\n",
    "    with open(label_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(yolo_lines))\n",
    "\n",
    "print(\"COCO annotations successfully converted to YOLO format!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c88865a-a72f-441b-a10f-7e6e3efe2306",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML file created at: /root/intruder_dataset/dataset.yaml\n"
     ]
    }
   ],
   "source": [
    "# Define the YAML content as a string\n",
    "yaml_content = \"\"\"\n",
    "path: ~/intruder_dataset\n",
    "train: images/train\n",
    "val: images/val\n",
    "\n",
    "nc: 6\n",
    "names: [ \"person\", \"backpack\", \"handbag\", \"suitcase\", \"cell phone\", \"laptop\" ]\n",
    "\"\"\"\n",
    "\n",
    "# Set the path\n",
    "yaml_path = os.path.expanduser(\"~/intruder_dataset/dataset.yaml\")\n",
    "\n",
    "# Write the content to the YAML file\n",
    "with open(yaml_path, \"w\") as yaml_file:\n",
    "    yaml_file.write(yaml_content)\n",
    "\n",
    "print(f\"YAML file created at: {yaml_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1cf9fca-84a0-49cf-a0e5-136177eda682",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images and labels split into train/val folders.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "base_dir = os.path.expanduser('~/intruder_dataset')\n",
    "img_dir = os.path.join(base_dir, 'images')\n",
    "label_dir = os.path.join(base_dir, 'labels')\n",
    "\n",
    "# Make train/val folders\n",
    "for split in ['train', 'val']:\n",
    "    os.makedirs(os.path.join(img_dir, split), exist_ok=True)\n",
    "    os.makedirs(os.path.join(label_dir, split), exist_ok=True)\n",
    "\n",
    "# List all images\n",
    "images = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]\n",
    "random.shuffle(images)\n",
    "\n",
    "split_idx = int(0.8 * len(images))  # 80% for training\n",
    "train_imgs = images[:split_idx]\n",
    "val_imgs = images[split_idx:]\n",
    "\n",
    "# Move files\n",
    "for img in train_imgs:\n",
    "    shutil.move(os.path.join(img_dir, img), os.path.join(img_dir, 'train', img))\n",
    "    shutil.move(os.path.join(label_dir, img.replace('.jpg', '.txt')), os.path.join(label_dir, 'train', img.replace('.jpg', '.txt')))\n",
    "\n",
    "for img in val_imgs:\n",
    "    shutil.move(os.path.join(img_dir, img), os.path.join(img_dir, 'val', img))\n",
    "    shutil.move(os.path.join(label_dir, img.replace('.jpg', '.txt')), os.path.join(label_dir, 'val', img.replace('.jpg', '.txt')))\n",
    "\n",
    "print(\"Images and labels split into train/val folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfc0a3f-635a-4aba-9b5d-17398bb17315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8124456f-3c81-4d9e-adae-0771ad3a6acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee70a298-e800-4198-8f06-335d61c48da5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading COCO annotations...\n",
      "Filtering annotations by target category IDs...\n",
      "Matched annotations for 67910 images.\n",
      "Converting to YOLO format and writing .txt files...\n",
      "\n",
      " Done! 67910 YOLO annotation files created with 301135 total annotations.\n",
      "Saved to: /root/intruder_dataset/modified_annotation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Setup\n",
    "base_dir = os.path.expanduser(\"~/intruder_dataset\")\n",
    "img_dir = os.path.join(base_dir, \"images\")\n",
    "ann_dir = os.path.join(base_dir, \"annotations\")\n",
    "label_dir = os.path.join(base_dir, \"modified_annotation\")\n",
    "os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "# Mapping COCO â†’ YOLO class index (for COCO-trained models)\n",
    "\n",
    "coco_to_yolo_index = {\n",
    "    1: 0, 27: 1, 31: 2, 33: 3, 77: 4, 73: 5\n",
    "}\n",
    "# Load annotations\n",
    "print(\"Loading COCO annotations...\")\n",
    "with open(os.path.join(ann_dir, \"annotations/instances_train2017.json\")) as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# Filter annotations and group by image\n",
    "print(\"Filtering annotations by target category IDs...\")\n",
    "target_ids = set(coco_to_yolo_index.keys())\n",
    "ann_map = defaultdict(list)\n",
    "for ann in coco[\"annotations\"]:\n",
    "    if ann[\"category_id\"] in target_ids:\n",
    "        ann_map[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "print(f\"Matched annotations for {len(ann_map)} images.\")\n",
    "\n",
    "# Map image ID to metadata\n",
    "image_map = {img[\"id\"]: img for img in coco[\"images\"]}\n",
    "target_images = [image_map[i] for i in ann_map]\n",
    "\n",
    "# Convert and write to YOLO format\n",
    "print(\"Converting to YOLO format and writing .txt files...\")\n",
    "file_count = 0\n",
    "ann_count = 0\n",
    "\n",
    "for img in target_images:\n",
    "    img_id = img[\"id\"]\n",
    "    width, height = img[\"width\"], img[\"height\"]\n",
    "    file_stem = os.path.splitext(img[\"file_name\"])[0]\n",
    "\n",
    "    lines = []\n",
    "    for ann in ann_map[img_id]:\n",
    "        class_id = coco_to_yolo_index[ann[\"category_id\"]]\n",
    "        x, y, w, h = ann[\"bbox\"]\n",
    "        x_center = (x + w / 2) / width\n",
    "        y_center = (y + h / 2) / height\n",
    "        w_norm = w / width\n",
    "        h_norm = h / height\n",
    "        lines.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\")\n",
    "        ann_count += 1\n",
    "\n",
    "    with open(os.path.join(label_dir, file_stem + \".txt\"), \"w\") as f_out:\n",
    "        f_out.write(\"\\n\".join(lines))\n",
    "    file_count += 1\n",
    "\n",
    "print(f\"\\n Done! {file_count} YOLO annotation files created with {ann_count} total annotations.\")\n",
    "print(f\"Saved to: {label_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9f2efed-c512-41e4-9e09-d8a4182f8e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Project\n",
      "total 2216\n",
      "drwxr-xr-x 3 root root    4096 May  4 16:37 annotations\n",
      "drwxr-xr-x 2 root root    4096 May  4 16:38 images\n",
      "drwxr-xr-x 2 root root 2256896 May  4 18:09 modified_annotations\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls -l ~/intruder_dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2910403-9c9d-4446-b5df-90eb005a9b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved to Jupyter folder: /Project/modified_annotation\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "src = os.path.expanduser(\"~/intruder_dataset/modified_annotation\")\n",
    "dst = os.path.join(os.getcwd(), \"modified_annotation\")\n",
    "\n",
    "shutil.move(src, dst)\n",
    "print(f\"Moved to Jupyter folder: {dst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89580ac7-e6e7-4120-b5ed-8afefbe47056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Class IDs found in annotation files: [0, 1, 2, 3, 4, 5]\n",
      "ðŸ”¢ Maximum class ID: 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to your YOLO annotations\n",
    "label_dir = os.path.expanduser(\"/Project/modified_annotation\")  # or your actual folder\n",
    "\n",
    "max_class_id = -1\n",
    "class_ids_found = set()\n",
    "\n",
    "# Loop through all label files\n",
    "for file in os.listdir(label_dir):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join(label_dir, file)) as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if parts:\n",
    "                    class_id = int(parts[0])\n",
    "                    class_ids_found.add(class_id)\n",
    "                    max_class_id = max(max_class_id, class_id)\n",
    "\n",
    "print(\"âœ… Class IDs found in annotation files:\", sorted(class_ids_found))\n",
    "print(\"ðŸ”¢ Maximum class ID:\", max_class_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d81bb336-fb9a-4b3b-8165-6fe1cdb97cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Annotations split and copied based on image folders.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "base_dir = os.path.expanduser('/intruder_dataset')\n",
    "img_dir = os.path.join(base_dir, 'images')\n",
    "label_dir = os.path.join(base_dir, 'labels')\n",
    "src_ann_dir = '/Project/modified_annotation'  # source of your fixed annotations\n",
    "# 1. Clear existing label folders\n",
    "for split in ['train', 'val']:\n",
    "    split_dir = os.path.join(label_dir, split)\n",
    "    if os.path.exists(split_dir):\n",
    "        for f in os.listdir(split_dir):\n",
    "            f_path = os.path.join(split_dir, f)\n",
    "            if os.path.isfile(f_path):\n",
    "                os.remove(f_path)\n",
    "    else:\n",
    "        os.makedirs(split_dir)\n",
    "\n",
    "# 2. Copy annotations based on image names\n",
    "for split in ['train', 'val']:\n",
    "    img_split_dir = os.path.join(img_dir, split)\n",
    "    label_split_dir = os.path.join(label_dir, split)\n",
    "\n",
    "    for fname in os.listdir(img_split_dir):\n",
    "        if fname.endswith('.jpg'):\n",
    "            label_file = fname.replace('.jpg', '.txt')\n",
    "            src_label_path = os.path.join(src_ann_dir, label_file)\n",
    "            dst_label_path = os.path.join(label_split_dir, label_file)\n",
    "\n",
    "            if os.path.exists(src_label_path):\n",
    "                shutil.copy(src_label_path, dst_label_path)\n",
    "\n",
    "print(\"âœ… Annotations split and copied based on image folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b324c-8756-4e99-81c0-52c27e350179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
